{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install torch torchvision torchaudio transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "from torch.quantization import quantize_dynamic\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nome_modelo = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(nome_modelo)\n",
        "modelo = AutoModelForSequenceClassification.from_pretrained(nome_modelo)\n",
        "modelo.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "texto = \"Eu amo a Universidade Federal Fluminense!\"\n",
        "inputs = tokenizer(texto, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = modelo(**inputs)\n",
        "\n",
        "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(\"Probabilidades (modelo original):\", probs.numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modelo_quantizado = quantize_dynamic(\n",
        "    modelo,\n",
        "    {torch.nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    outputs_q = modelo_quantizado(**inputs)\n",
        "\n",
        "probs_q = torch.nn.functional.softmax(outputs_q.logits, dim=-1)\n",
        "print(\"Probabilidades (modelo quantizado):\", probs_q.numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(modelo.state_dict(), \"original.pt\")\n",
        "torch.save(modelo_quantizado.state_dict(), \"quantizado.pt\")\n",
        "\n",
        "tamanho_original = os.path.getsize(\"original.pt\") / 1e6\n",
        "tamanho_quantizado = os.path.getsize(\"quantizado.pt\") / 1e6\n",
        "\n",
        "print(f\"Tamanho original: {tamanho_original:.2f} MB\")\n",
        "print(f\"Tamanho quantizado: {tamanho_quantizado:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "texto = \"Texto para teste de performance sobre os benefícios da quantização.\"\n",
        "inputs = tokenizer(texto, return_tensors=\"pt\")\n",
        "\n",
        "start_time = time.time()\n",
        "for _ in range(100):\n",
        "    with torch.no_grad():\n",
        "        modelo(**inputs)\n",
        "tempo_original = (time.time() - start_time) / 100\n",
        "\n",
        "start_time = time.time()\n",
        "for _ in range(100):\n",
        "    with torch.no_grad():\n",
        "        modelo_quantizado(**inputs)\n",
        "tempo_quantizado = (time.time() - start_time) / 100\n",
        "\n",
        "print(f\"\\nTempo de inferência original (média de 100 execuções): {tempo_original:.6f} segundos\")\n",
        "print(f\"Tempo de inferência quantizado (média de 100 execuções): {tempo_quantizado:.6f} segundos\")\n",
        "\n",
        "speedup = tempo_original / tempo_quantizado\n",
        "print(f\"O modelo quantizado é {speedup:.2f}x mais rápido.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Comparando a Saída dos Modelos ---\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    probs_original = torch.nn.functional.softmax(modelo(**inputs).logits, dim=-1)\n",
        "    probs_quantizado = torch.nn.functional.softmax(modelo_quantizado(**inputs).logits, dim=-1)\n",
        "\n",
        "print(\"Original:\", probs_original.numpy())\n",
        "print(\"Quantizado:\", probs_quantizado.numpy())\n",
        "\n",
        "diff = torch.abs(probs_original - probs_quantizado).sum().item()\n",
        "print(f\"Erro absoluto total: {diff:.6f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
